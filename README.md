# ğŸ§  Salient Object Detection (SOD) â€” Deep Learning Project

This project implements a **Salient Object Detection (SOD)** model using TensorFlow/Keras.  
The goal is to train a neural network that identifies the **most visually important region** in an image by generating a corresponding saliency mask.

The project includes:

- ğŸ“¦ Dataset loading & preprocessing  
- ğŸ§± Model architecture (U-Net + improvements)  
- ğŸ‹ï¸ Training pipeline with checkpoints  
- ğŸ“Š Evaluation (IoU, Precision, Recall, F1, MAE)  
- ğŸ–¼ï¸ Demo notebook for running predictions  

---

## ğŸ“‚ Project Structure

SOD/
â”‚â”€â”€ checkpoints/               # Baseline model checkpoints
â”‚â”€â”€ checkpoints_exps/          # Experiment model checkpoints
â”‚â”€â”€ ECSSD/                     # Dataset (images + masks)
â”‚â”€â”€ venv / venv_new / venv_tf  # Virtual environments (ignored via .gitignore)
â”‚
â”‚â”€â”€ data_loader.py             # Dataset loading + augmentations
â”‚â”€â”€ sod_model.py               # Baseline & improved U-Net architectures
â”‚â”€â”€ train.py                   # Training script
â”‚â”€â”€ evaluate.py                # Evaluation script (IoU, F1, Precision, Recall)
â”‚â”€â”€ visualize_and_compare.py   # Visualize GT vs baseline vs improved model
â”‚â”€â”€ run_eval.py                # Quick eval runner
â”‚â”€â”€ demo_notebook.py           # Demo script for predictions
â”‚
â”‚â”€â”€ experiments_summary.csv    # Table comparing baseline & improved models
â”‚â”€â”€ val_f1_comparison.png      # Plot comparing validation F1 curves
â”‚â”€â”€ requirements.txt           # Dependencies
â”‚â”€â”€ README.md                  # This file
â”‚â”€â”€ .gitignore




---

## ğŸ“ Dataset

Supported datasets:

- **ECSSD**
- **DUTS**
- **HKU-IS**

Expected folder structure:

ECSSD/
â”œâ”€â”€ images/
â””â”€â”€ ground_truth_mask/

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```


## Run the training script:

python train.py


This will:

Load the dataset

Split into train/validation/test

Train the U-Net model

Save the best model weights in checkpoints/



## Metrics:

IoU

Precision

Recall

F1-Score
##  Demo (Visualization)

Open:

demo_notebook.ipynb


Inside, you can:

Load a sample image

Predict its saliency mask

Visualize input, ground truth, prediction, and overlay

ğŸš€ Features
âœ”ï¸ Baseline U-Net

4 encoder blocks + bottleneck + 4 decoder blocks

Loss = BCE + Î±Â·(1 â€“ IoU)

Metrics: IoU, Precision, Recall, F1-score

## âœ”ï¸ Dataset Pipeline

Auto-pairing images & masks

Augmentations:

Random horizontal flip

Random brightness

Random rotation

tf.data with caching, batching, prefetching

## âœ”ï¸ Experiments Included

Two improvement experiments were run:

Experiment 1 â€” Add Dropout + BatchNorm

Improves generalization

Stabilizes training

## The best weights are saved automatically:

checkpoints/best_weights.weights.h5


## Improved model weights:

checkpoints_exps/best_weights_exp1.h5
checkpoints_exps/best_weights_exp2.h5

## ğŸ§ª Evaluation

Run on test set:

python run_eval.py
